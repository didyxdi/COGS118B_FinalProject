{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as sc\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import plotly.figure_factory as ff\n",
    "import plotly as py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import social factors\n",
    "soc_df = pd.read_csv(\"datasets/after/mergedSOC.csv\", index_col=0)\n",
    "print(soc_df.shape)\n",
    "soc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import hospital discharge data\n",
    "hd_df = pd.read_csv(\"datasets/after/mergedIP.csv\", index_col=0)\n",
    "print(hd_df.shape)\n",
    "hd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregression to Predict 2019 Social Factors\n",
    "\n",
    "### Insert Details\n",
    "\n",
    "For the first part of our project, we decided to use an autoregressive model to predict social factors in 2019 using the previous years.  We then compared this to the actual social data from 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergedSOC_df[[]]\n",
    "mergedSOC_df = pd.read_csv(\"datasets/after/mergedSOC.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factors is an array of dataframes where factors[i] describes the i-th factor.\n",
    "# For each dataframe, there are 51 rows (number of counties) and 7 columns (number of years)\n",
    "factors = []\n",
    "for i in range(0, 48):\n",
    "    factors.append(mergedSOC_df[[]])\n",
    "\n",
    "i = 0\n",
    "for c in mergedSOC_df.columns:\n",
    "    factors[i] = pd.concat([factors[i], mergedSOC_df.loc[:,c]], axis=1, join='inner')\n",
    "    factors[i] = factors[i].rename(columns={c: c[-2:]})\n",
    "    i = (i + 1) % 48\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test/train data\n",
    "test = factors[0][\"19\"]\n",
    "train = factors[0].drop(\"19\", axis=1)\n",
    "test = np.array(test)\n",
    "train = np.array(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run autoregression per factor\n",
    "predicts = []\n",
    "for i in range(51):\n",
    "        model = AutoReg(train[i], lags = 2)\n",
    "        fit_model = model.fit()\n",
    "        predict = fit_model.predict(start = len(train[0,:]), end = len(train[0,:]), dynamic = False)\n",
    "        predicts.append(predict)\n",
    "predicts = np.array(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize, calc error\n",
    "msqr = 1/len(test)*np.sum(test-predicts)**2\n",
    "rmsqr = msqr**(1/2)\n",
    "plt.plot(predicts)\n",
    "plt.plot(test)\n",
    "rmsqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear vs Linear Neural Net\n",
    "\n",
    "### Using Social Factors to predict Health Outcomes\n",
    "\n",
    "For the next part of our project, we wanted to use our social factors to predict hospital discharges.  We first classified each county as \"below average health\", \"average health\", and \"above average health\" depending on the percentage of discharges they had relative to their population.\n",
    "\n",
    "We then ran both a nonlinear neural network with sigmoid activation functions, and a linear network to compare how they functioned relative to one another.\n",
    "\n",
    "Each model was run with social data from one year as inputs, and health outcome prediction (below, average, above) on the same respective year where below average health corresponds to an output of $100$, average to $110$ and above to $111$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_features = hd_df.groupby(np.arange(len(hd_df.columns))//4, axis=1).mean()\n",
    "df_train_features.columns = [\"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\"]\n",
    "print(df_train_features.shape)\n",
    "df_train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate each year into its own df\n",
    "df_train_labels_2013 = df_train_features.iloc[:, 0]\n",
    "df_train_labels_2014 = df_train_features.iloc[:, 1]\n",
    "df_train_labels_2015 = df_train_features.iloc[:, 2]\n",
    "df_train_labels_2016 = df_train_features.iloc[:, 3]\n",
    "df_train_labels_2017 = df_train_features.iloc[:, 4]\n",
    "df_train_labels_2018 = df_train_features.iloc[:, 5]\n",
    "df_train_labels_2019 = df_train_features.iloc[:, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate .33 quantile for each year\n",
    "above_2013 = df_train_labels_2013.quantile(0.33)\n",
    "above_2014 = df_train_labels_2014.quantile(0.33)\n",
    "above_2015 = df_train_labels_2015.quantile(0.33)\n",
    "above_2016 = df_train_labels_2016.quantile(0.33)\n",
    "above_2017 = df_train_labels_2017.quantile(0.33)\n",
    "above_2018 = df_train_labels_2018.quantile(0.33)\n",
    "above_2019 = df_train_labels_2019.quantile(0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate .66 quantile for each year\n",
    "average_2013 = df_train_labels_2013.quantile(0.66)\n",
    "average_2014 = df_train_labels_2014.quantile(0.66)\n",
    "average_2015 = df_train_labels_2015.quantile(0.66)\n",
    "average_2016 = df_train_labels_2016.quantile(0.66)\n",
    "average_2017 = df_train_labels_2017.quantile(0.66)\n",
    "average_2018 = df_train_labels_2018.quantile(0.66)\n",
    "average_2019 = df_train_labels_2019.quantile(0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate 1 quantile for each year\n",
    "below_2013 = df_train_labels_2013.quantile(1)\n",
    "below_2014 = df_train_labels_2014.quantile(1)\n",
    "below_2015 = df_train_labels_2015.quantile(1)\n",
    "below_2016 = df_train_labels_2016.quantile(1)\n",
    "below_2017 = df_train_labels_2017.quantile(1)\n",
    "below_2018 = df_train_labels_2018.quantile(1)\n",
    "below_2019 = df_train_labels_2019.quantile(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_features_2013 = soc_df.iloc[:, 0:48]\n",
    "df_test_features_2014 = soc_df.iloc[:, 48:96]\n",
    "df_test_features_2015 = soc_df.iloc[:, 96:144]\n",
    "df_test_features_2016 = soc_df.iloc[:, 144:192]\n",
    "df_test_features_2017 = soc_df.iloc[:, 192:240]\n",
    "df_test_features_2018 = soc_df.iloc[:, 240:288]\n",
    "df_test_features_2019 = soc_df.iloc[:, 288:336]\n",
    "df_test_features_2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data into two data frames\n",
    "\n",
    "# First rename all social factor columns to blank so they combine correctly\n",
    "df_test_features_2013.columns = [''] * len(df_test_features_2013.columns)\n",
    "df_test_features_2014.columns = [''] * len(df_test_features_2013.columns)\n",
    "df_test_features_2015.columns = [''] * len(df_test_features_2013.columns)\n",
    "df_test_features_2016.columns = [''] * len(df_test_features_2013.columns)\n",
    "df_test_features_2017.columns = [''] * len(df_test_features_2013.columns)\n",
    "df_test_features_2018.columns = [''] * len(df_test_features_2013.columns)\n",
    "df_test_features_2019.columns = [''] * len(df_test_features_2013.columns)\n",
    "\n",
    "# New df_Factors has all the info appended so years are staked on top of each other not in the same row\n",
    "df_factors = df_test_features_2013.append(df_test_features_2014)\n",
    "df_factors = df_factors.append(df_test_features_2015)\n",
    "#df_factors = df_factors.append(df_test_features_2016)\n",
    "df_factors = df_factors.append(df_test_features_2017)\n",
    "df_factors = df_factors.append(df_test_features_2018)\n",
    "df_factors = df_factors.append(df_test_features_2019)\n",
    "\n",
    "# New df_Labels has all the health data for the counties\n",
    "labels = [df_train_labels_2013,df_train_labels_2014,df_train_labels_2015,#df_train_labels_2016,\n",
    "          df_train_labels_2017,df_train_labels_2018,df_train_labels_2019]\n",
    "df_labels = pd.concat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(df_factors,df_labels,test_size=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # scaling data may be important\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = MLPRegressor(activation = 'logistic', solver = 'adam', max_iter = 10000)\n",
    "NN.fit(df_factors,df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with Linear activation function\n",
    "NN_linear = MLPRegressor (activation = 'identity', solver = 'adam', max_iter = 10000)\n",
    "NN_linear.fit(df_factors,df_labels)\n",
    "NN_linear.score(df_test_features_2016,df_train_labels_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NN.score(df_test_features_2016, df_train_labels_2016))\n",
    "predicted = NN.predict(df_test_features_2016)\n",
    "#print(type(test_y), type(predicted))\n",
    "p = pd.DataFrame(predicted)\n",
    "p.index = df_train_labels_2016.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_health = pd.concat([p, df_train_labels_2016.to_frame()], axis=1)\n",
    "print(df_health)\n",
    "df_health.columns = ['predicted', 'actual']\n",
    "df_health_classified = pd.DataFrame(columns = ['predicted', 'actual'])\n",
    "for i, row in df_health.iterrows():\n",
    "    p = row[0]\n",
    "    a = row[1]\n",
    "    if p < above_2013:\n",
    "        p = 'Above'\n",
    "    elif p < average_2013:\n",
    "        p = 'Average'\n",
    "    else:\n",
    "        p = 'Below'\n",
    "    df_health_classified.at[i,'predicted'] = p\n",
    "    if a < above_2013:\n",
    "        a = 'Above'\n",
    "    elif a < average_2013:\n",
    "        a = 'Average'\n",
    "    else:\n",
    "        a = 'Below'\n",
    "    df_health_classified.at[i,'actual'] = a\n",
    "df_health_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health = pd.concat([p, df_train_labels_2016.to_frame()], axis=1)\n",
    "df_health.columns = ['predicted', 'actual']\n",
    "df_health_classified = pd.DataFrame(columns = ['predicted', 'actual'])\n",
    "for i, row in df_health.iterrows():\n",
    "    p = row[0]\n",
    "    a = row[1]\n",
    "    if p < above_2013:\n",
    "        p = 1\n",
    "    elif p < average_2013:\n",
    "        p = 2\n",
    "    else:\n",
    "        p = 3\n",
    "    df_health_classified.at[i,'predicted'] = p\n",
    "    if a < above_2013:\n",
    "        a = 1\n",
    "    elif a < average_2013:\n",
    "        a = 2\n",
    "    else:\n",
    "        a = 3\n",
    "    df_health_classified.at[i,'actual'] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health_classified.index.names= ['county']\n",
    "\n",
    "df_health_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/minoritymajority.csv')\n",
    "df_sample_r = df_sample[df_sample['STNAME'] == 'Texas']\n",
    "\n",
    "counties = pd.read_csv('datasets/after/county.csv')\n",
    "counties.columns = ['i', 'FIPS', 'county']\n",
    "vis = pd.merge(df_health_classified, counties, on=\"county\")\n",
    "\n",
    "values1 = vis['predicted']\n",
    "values2 = vis['actual']\n",
    "\n",
    "endpts1 = list(np.mgrid[min(values1):max(values1):4j])\n",
    "endpts2 = list(np.mgrid[min(values2):max(values2):4j])\n",
    "\n",
    "#colorscale = [\"#030512\",\"#1d1d3b\",\"#323268\",\"#3d4b94\",\"#3e6ab0\",\"#4989bc\",\"#60a7c7\",\"#85c5d3\",\"#b7e0e4\",\"#eafcfd\"]\n",
    "#colorscale = [\"#f7fbff\", \"#ebf3fb\", \"#d2e3f3\", \"#c6dbef\", \"#b3d2e9\", \"#9ecae1\", \"#85bcdb\", \"#6baed6\", \"#57a0ce\", \"#3082be\", \"#2171b5\", \"#1361a9\",\"#08519c\", \"#0b4083\", \"#08306b\"]\n",
    "colorscale = [\"lightyellow\",\"orange\",\"orange\",\"red\",\"darkred\"]\n",
    "\n",
    "\n",
    "fips = vis['FIPS']\n",
    "\n",
    "fig = ff.create_choropleth(fips=fips, values=values1, scope=['Texas'], colorscale = colorscale, binning_endpoints=endpts1,\n",
    "        plot_bgcolor='rgb(100,100,100)',\n",
    "        paper_bgcolor='rgb(229,229,229)', county_outline={'color': 'rgb(255,255,255)', 'width': 0.75})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffles column x\n",
    "x = 0\n",
    "df_shuffled = np.random.permutation(df_factors.iloc[:,x].values)\n",
    "df_temp = df_factors.copy()\n",
    "df_temp.iloc[:,x] = df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data, haven't looked into exactly what this does but I saw that you should do this on some site\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_factors,df_labels,test_size=0.2, random_state=10)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for x in range(0,48): # computes an average score shuffling each column one by one and runing the program 25 times\n",
    "    average = df_factors.iloc[:,x].mean()\n",
    "    df_temp = df_factors.copy()\n",
    "    df_temp.iloc[:,x] = average\n",
    "    for i in range(0,500):\n",
    "        NN = MLPRegressor(activation = 'logistic', solver = 'adam', max_iter = 10000)\n",
    "        train_x, test_x, train_y, test_y = train_test_split(df_temp,df_labels,test_size=0.1, random_state=10)\n",
    "        NN.fit(train_x,train_y)\n",
    "        total.append(NN.score(test_x, test_y))\n",
    "for i in range(0,500):\n",
    "    NN = MLPRegressor(activation = 'logistic', solver = 'adam', max_iter = 10000)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df_factors,df_labels,test_size=0.1, random_state=10)\n",
    "    NN.fit(train_x,train_y)\n",
    "    total.append(NN.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_Results = pd.DataFrame(np.array(total).reshape(-1,500))\n",
    "df_test_Results['average'] = df_test_Results.mean(numeric_only=True, axis=1)\n",
    "df_test_Results['STDev'] = df_test_Results.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = list(\"Col{0}\".format(i) for i in range(1,49))\n",
    "x_axis.append(\"Complete\")\n",
    "x_pos = np.arange(len(x_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos = [0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, df_test_Results.loc[:,\"average\"], yerr=df_test_Results.loc[:,\"STDev\"])#, align='center',alpha=0.5, ecolor='black', capsize=2)\n",
    "ax.set_ylabel('Mean Score')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(x_axis)\n",
    "ax.set_title('Computing Average')\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No change\n",
    ".73\n",
    "0\n",
    "0.7108650254858128\n",
    "1\n",
    "0.7960634235499481\n",
    "2\n",
    "0.8294445453849859\n",
    "3\n",
    "0.7131161077022256\n",
    "4\n",
    "0.7649890226507019\n",
    "5\n",
    "0.7182807190899667\n",
    "6\n",
    "0.770573975108926\n",
    "7\n",
    "0.7678379400275417\n",
    "8\n",
    "0.7394518440306926\n",
    "9\n",
    "0.7370813452666014\n",
    "10\n",
    "0.7469239702406627\n",
    "11\n",
    "0.6987987125400841\n",
    "12\n",
    "0.8031039579147157\n",
    "13\n",
    "0.7597713988573509\n",
    "14\n",
    "0.7407542084947817\n",
    "15\n",
    "0.7551527617051675\n",
    "16\n",
    "0.7032623511716416\n",
    "17\n",
    "0.7468654275975656\n",
    "18\n",
    "0.7269607583883346\n",
    "19\n",
    "0.7239341835447677\n",
    "20\n",
    "0.7063222378120773\n",
    "21\n",
    "0.7515546917564759\n",
    "22\n",
    "0.7811960516713465\n",
    "23\n",
    "0.7875212552563348\n",
    "24\n",
    "0.7701103826676152\n",
    "25\n",
    "0.8098780049975205\n",
    "26\n",
    "0.7555997194214767\n",
    "27\n",
    "0.6816149802794327\n",
    "28\n",
    "0.7656032470712896\n",
    "29\n",
    "0.7524800447927893\n",
    "30\n",
    "0.7565911712933635\n",
    "31\n",
    "0.6411801245922919\n",
    "32\n",
    "0.6958630774881265\n",
    "33\n",
    "0.707291988941758\n",
    "34\n",
    "0.7003487106236796\n",
    "35\n",
    "0.7880963245934186\n",
    "36\n",
    "0.6818116911936285\n",
    "37\n",
    "0.6882282528159364\n",
    "38\n",
    "0.755357675227565\n",
    "39\n",
    "0.7848156225992295\n",
    "40\n",
    "0.6928275829955649\n",
    "41\n",
    "0.7067307441389035\n",
    "42\n",
    "0.7815071724434892\n",
    "43\n",
    "0.673067085455601\n",
    "44\n",
    "0.7333788145720775\n",
    "45\n",
    "0.6352091856558159\n",
    "46\n",
    "0.6732250725388015\n",
    "47\n",
    "0.7716650884613394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_Results.to_csv(\"totals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_best_factors = df_factors.iloc[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,\n",
    " #                                   33,34,35,36,37,38,39,40,41,42,43,44,45,46,47]]\n",
    "df_best_factors = df_factors.iloc[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,21,23,26,27,29,30,31,\n",
    "                                    33,34,35,36,37,38,39,40,41,42,43,44,45,46,47]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,500):\n",
    "    NN = MLPRegressor(activation = 'logistic', solver = 'adam', max_iter = 10000)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df_best_factors,df_labels,test_size=0.1, random_state=10)\n",
    "    NN.fit(train_x,train_y)\n",
    "    total = total + NN.score(test_x, test_y)\n",
    "print (total/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(df_factors,df_labels,test_size=0.2, random_state=10)\n",
    "NN = MLPRegressor(activation = 'relu', solver = 'adam', max_iter = 10000)\n",
    "NN.fit(train_x,train_y)\n",
    "NN.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0\n",
    "0.7477366611943289\n",
    "1\n",
    "0.7450918872678572\n",
    "2\n",
    "0.7365371215672839\n",
    "3\n",
    "0.7353415290661098\n",
    "4\n",
    "0.7157993397891446\n",
    "5\n",
    "0.7414025627768244\n",
    "6\n",
    "0.7432139691857386\n",
    "7\n",
    "0.7299629155090428\n",
    "8\n",
    "0.7430509940365544\n",
    "9\n",
    "0.7431590397923593\n",
    "10\n",
    "0.7379941614264255\n",
    "11\n",
    "0.7183108497078422\n",
    "12\n",
    "0.7359672553310749\n",
    "13\n",
    "0.7515365648653871\n",
    "14\n",
    "0.7327615548058445\n",
    "15\n",
    "0.7367755194065221\n",
    "16\n",
    "0.7122104338543712\n",
    "17\n",
    "0.7350669415210602\n",
    "18\n",
    "0.7351774821246562\n",
    "19\n",
    "0.7211025993383763\n",
    "20\n",
    "0.7580289523924054\n",
    "21\n",
    "0.7279603370575597\n",
    "22\n",
    "0.7554106271592013\n",
    "23\n",
    "0.7333225437211993\n",
    "24\n",
    "0.7618337793902102\n",
    "25\n",
    "0.7522192641169432\n",
    "26\n",
    "0.7212447362368076\n",
    "27\n",
    "0.7132314167375634\n",
    "28\n",
    "0.7522934951601369\n",
    "29\n",
    "0.7494191251460175\n",
    "30\n",
    "0.7270102273600064\n",
    "31\n",
    "0.7442020408419321\n",
    "32\n",
    "0.750297162703244\n",
    "33\n",
    "0.7004031627725568\n",
    "34\n",
    "0.7248352585915442\n",
    "35\n",
    "0.740584523382293\n",
    "36\n",
    "0.7282683443766068\n",
    "37\n",
    "0.73456833177976\n",
    "38\n",
    "0.7349586841533234\n",
    "39\n",
    "0.7307063595104687\n",
    "40\n",
    "0.7469908871969103\n",
    "41\n",
    "0.7368946729741306\n",
    "42\n",
    "0.7465210991017219\n",
    "43\n",
    "0.7188909451460147\n",
    "44\n",
    "0.7382468534620368\n",
    "45\n",
    "0.7348314689406404\n",
    "46\n",
    "0.7237537620249058\n",
    "47\n",
    "0.7447635388268783\n",
    "0.7486788908834031"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
